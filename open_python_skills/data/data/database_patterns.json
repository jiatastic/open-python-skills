{
  "source": "Curated + SQLAlchemy/Alembic common patterns",
  "version": "2026-01",
  "description": "Database patterns for SQLAlchemy and Alembic",
  "entries": [
    {
      "id": "db-sqlalchemy-async-engine",
      "category": "database",
      "title": "SQLAlchemy 2.0 Async Engine + Session",
      "tags": [
        "database",
        "sqlalchemy",
        "async",
        "session",
        "engine"
      ],
      "summary": "Use async engine + async session per request; close sessions reliably.",
      "content": "Create a single async engine for the app. Provide an AsyncSession via dependency injection with proper cleanup. Use expire_on_commit=False for async sessions to avoid implicit IO after commit.",
      "code_examples": [
        {
          "description": "Async engine + session dependency",
          "code": "from collections.abc import AsyncGenerator\n\nfrom sqlalchemy.ext.asyncio import AsyncSession, async_sessionmaker, create_async_engine\n\nDATABASE_URL = \"postgresql+asyncpg://user:pass@localhost:5432/app\"\n\nengine = create_async_engine(DATABASE_URL, pool_pre_ping=True)\nSessionLocal = async_sessionmaker(engine, expire_on_commit=False)\n\nasync def get_session() -> AsyncGenerator[AsyncSession, None]:\n    async with SessionLocal() as session:\n        yield session\n"
        }
      ]
    },
    {
      "id": "db-unit-of-work-commit-rollback",
      "category": "database",
      "title": "Commit/Rollback Pattern",
      "tags": [
        "database",
        "sqlalchemy",
        "transaction",
        "commit",
        "rollback"
      ],
      "summary": "Commit on success, rollback on errors. Keep transaction boundaries clear.",
      "content": "Wrap write operations in try/except, rollback on exceptions. Avoid committing in deep helper functions unless explicit. Use context managers for clean transaction boundaries.",
      "code_examples": [
        {
          "description": "Safe transaction block",
          "code": "from sqlalchemy.ext.asyncio import AsyncSession\n\nasync def create_user(session: AsyncSession, user: dict) -> User:\n    try:\n        db_user = User(**user)\n        session.add(db_user)\n        await session.commit()\n        await session.refresh(db_user)\n        return db_user\n    except Exception:\n        await session.rollback()\n        raise\n"
        },
        {
          "description": "Nested transaction with savepoint",
          "code": "async def transfer_funds(session: AsyncSession, from_id: int, to_id: int, amount: float):\n    async with session.begin_nested():  # Creates a savepoint\n        from_account = await session.get(Account, from_id)\n        to_account = await session.get(Account, to_id)\n        \n        if from_account.balance < amount:\n            raise InsufficientFunds()\n        \n        from_account.balance -= amount\n        to_account.balance += amount\n    \n    await session.commit()\n"
        }
      ]
    },
    {
      "id": "db-alembic-file-template",
      "category": "database",
      "title": "Alembic Migration Naming Template",
      "tags": [
        "database",
        "alembic",
        "migrations",
        "naming"
      ],
      "summary": "Use a human-readable migration file template (date + slug).",
      "content": "Configure Alembic to generate readable migration filenames to improve review and operations. Include date for chronological ordering and descriptive slug.",
      "code_examples": [
        {
          "description": "alembic.ini template",
          "code": "# alembic.ini\nfile_template = %%(year)d-%%(month).2d-%%(day).2d_%%(slug)s\n# Results in: 2024-08-24_add_users_table.py\n"
        }
      ]
    },
    {
      "id": "db-naming-conventions-metadata",
      "category": "database",
      "title": "DB Constraint Naming Conventions",
      "tags": [
        "database",
        "sqlalchemy",
        "metadata",
        "conventions",
        "postgres"
      ],
      "summary": "Set naming conventions for indexes/constraints for stable migrations.",
      "content": "Define a naming_convention on SQLAlchemy MetaData so Alembic autogenerate remains stable and consistent. This ensures predictable constraint names across environments.",
      "code_examples": [
        {
          "description": "Postgres naming convention",
          "code": "from sqlalchemy import MetaData\n\nNAMING_CONVENTION = {\n    \"ix\": \"%(column_0_label)s_idx\",\n    \"uq\": \"%(table_name)s_%(column_0_name)s_key\",\n    \"ck\": \"%(table_name)s_%(constraint_name)s_check\",\n    \"fk\": \"%(table_name)s_%(column_0_name)s_fkey\",\n    \"pk\": \"%(table_name)s_pkey\",\n}\n\nmetadata = MetaData(naming_convention=NAMING_CONVENTION)\n"
        }
      ]
    },
    {
      "id": "db-eager-loading-n-plus-one",
      "category": "database",
      "title": "Eager Loading - Avoid N+1 Queries",
      "tags": [
        "database",
        "sqlalchemy",
        "eager-loading",
        "n+1",
        "selectinload",
        "joinedload"
      ],
      "summary": "Use selectinload or joinedload to prevent N+1 query problems.",
      "content": "N+1 occurs when accessing lazy-loaded relationships causes a separate query per object. Use selectinload for collections (separate IN query) or joinedload for single objects (JOIN). Use raiseload to detect unwanted lazy loading in development.",
      "code_examples": [
        {
          "description": "selectinload for collections",
          "code": "from sqlalchemy import select\nfrom sqlalchemy.orm import selectinload\n\n# Load all users with their orders in 2 queries (users + orders IN)\nstmt = select(User).options(selectinload(User.orders))\nusers = await session.scalars(stmt)\n\nfor user in users:\n    # No additional query - orders already loaded\n    print(user.orders)\n"
        },
        {
          "description": "joinedload for single relationships",
          "code": "from sqlalchemy.orm import joinedload\n\n# Load orders with their user in 1 query (JOIN)\nstmt = select(Order).options(joinedload(Order.user))\norders = await session.scalars(stmt)\n\nfor order in orders:\n    # No additional query - user already loaded\n    print(order.user.name)\n"
        },
        {
          "description": "Chained eager loading",
          "code": "from sqlalchemy.orm import selectinload\n\n# Load User -> Orders -> Items in 3 queries\nstmt = select(User).options(\n    selectinload(User.orders).selectinload(Order.items)\n)\n"
        },
        {
          "description": "raiseload to detect N+1 in development",
          "code": "from sqlalchemy.orm import raiseload\n\n# Raises error if any relationship is accessed that wasn't eager loaded\nstmt = select(User).options(\n    selectinload(User.orders),\n    raiseload(\"*\")  # Raise on any other lazy load\n)\n"
        }
      ]
    },
    {
      "id": "db-cascade-delete",
      "category": "database",
      "title": "Cascade Delete - ORM and Database Level",
      "tags": [
        "database",
        "sqlalchemy",
        "cascade",
        "delete",
        "foreign-key"
      ],
      "summary": "Configure cascade deletes at both ORM and database level for consistency.",
      "content": "Use ORM cascade='all, delete' for application-level cascades. Add ondelete='CASCADE' on ForeignKey for database-level enforcement. Use passive_deletes=True to let the database handle deletion without loading collections.",
      "code_examples": [
        {
          "description": "ORM + Database cascade delete",
          "code": "from sqlalchemy import ForeignKey, Integer, String\nfrom sqlalchemy.orm import DeclarativeBase, Mapped, mapped_column, relationship\n\nclass Base(DeclarativeBase):\n    pass\n\nclass Parent(Base):\n    __tablename__ = \"parent\"\n    \n    id: Mapped[int] = mapped_column(primary_key=True)\n    name: Mapped[str] = mapped_column(String(100))\n    \n    # ORM cascade + let DB handle deletes\n    children: Mapped[list[\"Child\"]] = relationship(\n        back_populates=\"parent\",\n        cascade=\"all, delete\",\n        passive_deletes=True,\n    )\n\nclass Child(Base):\n    __tablename__ = \"child\"\n    \n    id: Mapped[int] = mapped_column(primary_key=True)\n    # Database-level cascade\n    parent_id: Mapped[int] = mapped_column(\n        ForeignKey(\"parent.id\", ondelete=\"CASCADE\")\n    )\n    \n    parent: Mapped[\"Parent\"] = relationship(back_populates=\"children\")\n"
        },
        {
          "description": "Many-to-many cascade",
          "code": "from sqlalchemy import Column, ForeignKey, Integer, Table\n\nassociation_table = Table(\n    \"association\",\n    Base.metadata,\n    Column(\"left_id\", Integer, ForeignKey(\"left.id\", ondelete=\"CASCADE\")),\n    Column(\"right_id\", Integer, ForeignKey(\"right.id\", ondelete=\"CASCADE\")),\n)\n\nclass Parent(Base):\n    __tablename__ = \"left\"\n    id: Mapped[int] = mapped_column(primary_key=True)\n    \n    children: Mapped[list[\"Child\"]] = relationship(\n        secondary=association_table,\n        back_populates=\"parents\",\n        cascade=\"all, delete\",\n    )\n\nclass Child(Base):\n    __tablename__ = \"right\"\n    id: Mapped[int] = mapped_column(primary_key=True)\n    \n    parents: Mapped[list[\"Parent\"]] = relationship(\n        secondary=association_table,\n        back_populates=\"children\",\n        passive_deletes=True,\n    )\n"
        }
      ]
    },
    {
      "id": "db-soft-delete",
      "category": "database",
      "title": "Soft Delete Pattern",
      "tags": [
        "database",
        "sqlalchemy",
        "soft-delete",
        "deleted_at",
        "audit"
      ],
      "summary": "Use a deleted_at timestamp instead of hard deletes for data recovery.",
      "content": "Soft deletes preserve data for auditing and recovery. Add a deleted_at column, filter it out in queries, and use a mixin for consistency. Consider using a hybrid property for is_deleted checks.",
      "code_examples": [
        {
          "description": "Soft delete mixin",
          "code": "from datetime import datetime\nfrom sqlalchemy import DateTime\nfrom sqlalchemy.orm import Mapped, mapped_column\nfrom sqlalchemy.ext.hybrid import hybrid_property\n\nclass SoftDeleteMixin:\n    deleted_at: Mapped[datetime | None] = mapped_column(\n        DateTime(timezone=True), \n        default=None,\n        index=True\n    )\n    \n    @hybrid_property\n    def is_deleted(self) -> bool:\n        return self.deleted_at is not None\n    \n    def soft_delete(self) -> None:\n        self.deleted_at = datetime.utcnow()\n    \n    def restore(self) -> None:\n        self.deleted_at = None\n\nclass User(Base, SoftDeleteMixin):\n    __tablename__ = \"user\"\n    id: Mapped[int] = mapped_column(primary_key=True)\n    name: Mapped[str] = mapped_column(String(100))\n"
        },
        {
          "description": "Filter soft-deleted in queries",
          "code": "from sqlalchemy import select\n\n# Only get non-deleted users\nstmt = select(User).where(User.deleted_at.is_(None))\n\n# Include deleted (for admin)\nstmt_all = select(User)\n\n# Only deleted\nstmt_deleted = select(User).where(User.deleted_at.isnot(None))\n"
        },
        {
          "description": "Automatic filter with event listener",
          "code": "from sqlalchemy import event\nfrom sqlalchemy.orm import Query\n\n@event.listens_for(Query, \"before_compile\", retval=True)\ndef filter_soft_deleted(query):\n    for desc in query.column_descriptions:\n        entity = desc.get(\"entity\")\n        if entity and hasattr(entity, \"deleted_at\"):\n            query = query.filter(entity.deleted_at.is_(None))\n    return query\n"
        }
      ]
    },
    {
      "id": "db-optimistic-locking",
      "category": "database",
      "title": "Optimistic Locking with Version Column",
      "tags": [
        "database",
        "sqlalchemy",
        "optimistic-locking",
        "version",
        "concurrency"
      ],
      "summary": "Use version_id_col for optimistic concurrency control to prevent lost updates.",
      "content": "Optimistic locking detects concurrent modifications by checking a version number. SQLAlchemy includes the version in UPDATE WHERE clause - if no rows match, StaleDataError is raised. Use for records with potential concurrent edits.",
      "code_examples": [
        {
          "description": "Integer version counter",
          "code": "from sqlalchemy import Integer, String\nfrom sqlalchemy.orm import Mapped, mapped_column\n\nclass Article(Base):\n    __tablename__ = \"article\"\n    \n    id: Mapped[int] = mapped_column(primary_key=True)\n    title: Mapped[str] = mapped_column(String(200))\n    content: Mapped[str] = mapped_column(String)\n    version_id: Mapped[int] = mapped_column(Integer, nullable=False, default=1)\n    \n    __mapper_args__ = {\n        \"version_id_col\": version_id\n    }\n\n# Usage - StaleDataError raised if version changed\narticle = await session.get(Article, 1)\narticle.title = \"Updated Title\"\nawait session.commit()  # Checks version_id in WHERE clause\n"
        },
        {
          "description": "Timestamp-based versioning",
          "code": "from datetime import datetime\nfrom sqlalchemy import DateTime\n\nclass Document(Base):\n    __tablename__ = \"document\"\n    \n    id: Mapped[int] = mapped_column(primary_key=True)\n    content: Mapped[str] = mapped_column(String)\n    updated_at: Mapped[datetime] = mapped_column(\n        DateTime(timezone=True),\n        nullable=False\n    )\n    \n    __mapper_args__ = {\n        \"version_id_col\": updated_at,\n        \"version_id_generator\": lambda v: datetime.utcnow()\n    }\n"
        },
        {
          "description": "Handle StaleDataError",
          "code": "from sqlalchemy.orm.exc import StaleDataError\n\nasync def update_article(session: AsyncSession, article_id: int, data: dict):\n    try:\n        article = await session.get(Article, article_id)\n        for key, value in data.items():\n            setattr(article, key, value)\n        await session.commit()\n    except StaleDataError:\n        await session.rollback()\n        raise ConcurrentModificationError(\n            \"Article was modified by another user. Please refresh and try again.\"\n        )\n"
        }
      ]
    },
    {
      "id": "db-timestamps-mixin",
      "category": "database",
      "title": "Automatic Timestamps (created_at, updated_at)",
      "tags": [
        "database",
        "sqlalchemy",
        "timestamps",
        "created_at",
        "updated_at",
        "mixin"
      ],
      "summary": "Use a mixin for automatic created_at and updated_at timestamps.",
      "content": "Add timestamp columns that automatically set on insert (created_at) and update (updated_at). Use server_default for database-level defaults. For updated_at, use onupdate or database triggers.",
      "code_examples": [
        {
          "description": "Timestamp mixin with Python defaults",
          "code": "from datetime import datetime\nfrom sqlalchemy import DateTime, func\nfrom sqlalchemy.orm import Mapped, mapped_column\n\nclass TimestampMixin:\n    created_at: Mapped[datetime] = mapped_column(\n        DateTime(timezone=True),\n        server_default=func.now(),\n        nullable=False\n    )\n    updated_at: Mapped[datetime] = mapped_column(\n        DateTime(timezone=True),\n        server_default=func.now(),\n        onupdate=func.now(),\n        nullable=False\n    )\n\nclass User(Base, TimestampMixin):\n    __tablename__ = \"user\"\n    id: Mapped[int] = mapped_column(primary_key=True)\n    name: Mapped[str] = mapped_column(String(100))\n"
        },
        {
          "description": "MySQL ON UPDATE CURRENT_TIMESTAMP",
          "code": "from sqlalchemy import text\nfrom sqlalchemy.schema import FetchedValue\n\nclass User(Base):\n    __tablename__ = \"user\"\n    \n    id: Mapped[int] = mapped_column(primary_key=True)\n    name: Mapped[str] = mapped_column(String(100))\n    \n    updated_at: Mapped[datetime] = mapped_column(\n        DateTime,\n        server_default=text(\"CURRENT_TIMESTAMP ON UPDATE CURRENT_TIMESTAMP\"),\n        server_onupdate=FetchedValue(),  # Fetch updated value from DB\n    )\n"
        }
      ]
    },
    {
      "id": "db-bulk-insert",
      "category": "database",
      "title": "Bulk Insert Operations",
      "tags": [
        "database",
        "sqlalchemy",
        "bulk",
        "insert",
        "performance"
      ],
      "summary": "Use bulk insert for efficient mass data insertion.",
      "content": "Bulk inserts are much faster than individual inserts. Use insert() with a list of dictionaries for best performance. Use RETURNING to get inserted objects back if needed.",
      "code_examples": [
        {
          "description": "Bulk insert with dictionaries",
          "code": "from sqlalchemy import insert\n\nasync def bulk_create_users(session: AsyncSession, users: list[dict]):\n    await session.execute(\n        insert(User),\n        [\n            {\"name\": \"Alice\", \"email\": \"alice@example.com\"},\n            {\"name\": \"Bob\", \"email\": \"bob@example.com\"},\n            {\"name\": \"Charlie\", \"email\": \"charlie@example.com\"},\n        ],\n    )\n    await session.commit()\n"
        },
        {
          "description": "Bulk insert with RETURNING",
          "code": "from sqlalchemy import insert\n\nasync def bulk_create_users(session: AsyncSession, users: list[dict]) -> list[User]:\n    result = await session.scalars(\n        insert(User).returning(User),\n        [\n            {\"name\": \"Alice\", \"email\": \"alice@example.com\"},\n            {\"name\": \"Bob\", \"email\": \"bob@example.com\"},\n        ],\n    )\n    await session.commit()\n    return result.all()\n"
        },
        {
          "description": "Chunked bulk insert for large datasets",
          "code": "from itertools import batched\nfrom sqlalchemy import insert\n\nasync def bulk_insert_chunked(\n    session: AsyncSession,\n    items: list[dict],\n    chunk_size: int = 1000\n):\n    for chunk in batched(items, chunk_size):\n        await session.execute(insert(Item), list(chunk))\n    await session.commit()\n"
        }
      ]
    },
    {
      "id": "db-bulk-update",
      "category": "database",
      "title": "Bulk Update Operations",
      "tags": [
        "database",
        "sqlalchemy",
        "bulk",
        "update",
        "performance"
      ],
      "summary": "Use bulk update for efficient mass updates without loading objects.",
      "content": "Bulk updates execute UPDATE statements directly without loading objects into the session. Much faster for updating many rows. Use synchronize_session='fetch' if you need session consistency.",
      "code_examples": [
        {
          "description": "Bulk update with WHERE clause",
          "code": "from sqlalchemy import update\n\nasync def deactivate_old_users(session: AsyncSession, days: int = 365):\n    from datetime import datetime, timedelta\n    \n    cutoff = datetime.utcnow() - timedelta(days=days)\n    \n    result = await session.execute(\n        update(User)\n        .where(User.last_login < cutoff)\n        .values(is_active=False)\n    )\n    await session.commit()\n    \n    return result.rowcount  # Number of rows updated\n"
        },
        {
          "description": "Bulk update with RETURNING",
          "code": "from sqlalchemy import update\n\nasync def increment_view_count(session: AsyncSession, post_ids: list[int]) -> list[Post]:\n    result = await session.scalars(\n        update(Post)\n        .where(Post.id.in_(post_ids))\n        .values(view_count=Post.view_count + 1)\n        .returning(Post)\n    )\n    await session.commit()\n    return result.all()\n"
        }
      ]
    },
    {
      "id": "db-upsert",
      "category": "database",
      "title": "Upsert (Insert or Update)",
      "tags": [
        "database",
        "sqlalchemy",
        "upsert",
        "on-conflict",
        "merge"
      ],
      "summary": "Use ON CONFLICT (PostgreSQL) or ON DUPLICATE KEY (MySQL) for upserts.",
      "content": "Upsert inserts a row if it doesn't exist, or updates it if it does. Use dialect-specific constructs for best performance. PostgreSQL uses ON CONFLICT, MySQL uses ON DUPLICATE KEY UPDATE.",
      "code_examples": [
        {
          "description": "PostgreSQL upsert with on_conflict_do_update",
          "code": "from sqlalchemy.dialects.postgresql import insert as pg_insert\n\nasync def upsert_user(session: AsyncSession, user_data: dict):\n    stmt = pg_insert(User).values(**user_data)\n    \n    stmt = stmt.on_conflict_do_update(\n        index_elements=[User.email],  # Conflict on unique email\n        set_={\n            \"name\": stmt.excluded.name,\n            \"updated_at\": func.now(),\n        }\n    )\n    \n    await session.execute(stmt)\n    await session.commit()\n"
        },
        {
          "description": "PostgreSQL upsert with RETURNING",
          "code": "from sqlalchemy.dialects.postgresql import insert as pg_insert\n\nasync def upsert_and_return(session: AsyncSession, data: dict) -> User:\n    stmt = (\n        pg_insert(User)\n        .values(**data)\n        .on_conflict_do_update(\n            index_elements=[User.email],\n            set_={\"name\": data[\"name\"]}\n        )\n        .returning(User)\n    )\n    \n    result = await session.scalar(stmt)\n    await session.commit()\n    return result\n"
        },
        {
          "description": "Bulk upsert",
          "code": "from sqlalchemy.dialects.postgresql import insert as pg_insert\n\nasync def bulk_upsert(session: AsyncSession, items: list[dict]):\n    stmt = pg_insert(Item).values(items)\n    \n    stmt = stmt.on_conflict_do_update(\n        index_elements=[Item.external_id],\n        set_={\n            \"name\": stmt.excluded.name,\n            \"price\": stmt.excluded.price,\n            \"updated_at\": func.now(),\n        }\n    )\n    \n    await session.execute(stmt)\n    await session.commit()\n"
        }
      ]
    },
    {
      "id": "db-index-strategies",
      "category": "database",
      "title": "Index Strategies",
      "tags": [
        "database",
        "sqlalchemy",
        "index",
        "performance",
        "composite"
      ],
      "summary": "Add indexes strategically for query performance.",
      "content": "Indexes speed up reads but slow down writes. Add indexes on columns used in WHERE, JOIN, and ORDER BY. Use composite indexes for multi-column conditions. Consider partial indexes for filtered queries.",
      "code_examples": [
        {
          "description": "Column-level index",
          "code": "from sqlalchemy import String, Index\nfrom sqlalchemy.orm import Mapped, mapped_column\n\nclass User(Base):\n    __tablename__ = \"user\"\n    \n    id: Mapped[int] = mapped_column(primary_key=True)\n    email: Mapped[str] = mapped_column(String(255), unique=True, index=True)\n    status: Mapped[str] = mapped_column(String(20), index=True)\n"
        },
        {
          "description": "Composite index",
          "code": "from sqlalchemy import Index\n\nclass Order(Base):\n    __tablename__ = \"order\"\n    \n    id: Mapped[int] = mapped_column(primary_key=True)\n    user_id: Mapped[int] = mapped_column(ForeignKey(\"user.id\"))\n    status: Mapped[str] = mapped_column(String(20))\n    created_at: Mapped[datetime] = mapped_column(DateTime)\n    \n    # Composite index for common query pattern\n    __table_args__ = (\n        Index(\"ix_order_user_status\", \"user_id\", \"status\"),\n        Index(\"ix_order_user_created\", \"user_id\", \"created_at\"),\n    )\n"
        },
        {
          "description": "Partial index (PostgreSQL)",
          "code": "from sqlalchemy import Index\n\nclass User(Base):\n    __tablename__ = \"user\"\n    \n    id: Mapped[int] = mapped_column(primary_key=True)\n    email: Mapped[str] = mapped_column(String(255))\n    is_active: Mapped[bool] = mapped_column(default=True)\n    \n    # Only index active users\n    __table_args__ = (\n        Index(\n            \"ix_user_email_active\",\n            \"email\",\n            unique=True,\n            postgresql_where=(is_active == True)\n        ),\n    )\n"
        }
      ]
    },
    {
      "id": "db-uuid-primary-key",
      "category": "database",
      "title": "UUID Primary Keys",
      "tags": [
        "database",
        "sqlalchemy",
        "uuid",
        "primary-key",
        "distributed"
      ],
      "summary": "Use UUIDs for primary keys in distributed systems or public-facing IDs.",
      "content": "UUIDs prevent ID enumeration, work in distributed systems, and can be generated client-side. Use uuid7 for sortable UUIDs. Consider using both internal integer ID and public UUID.",
      "code_examples": [
        {
          "description": "UUID primary key",
          "code": "import uuid\nfrom sqlalchemy import Uuid\nfrom sqlalchemy.orm import Mapped, mapped_column\n\nclass User(Base):\n    __tablename__ = \"user\"\n    \n    id: Mapped[uuid.UUID] = mapped_column(\n        Uuid,\n        primary_key=True,\n        default=uuid.uuid4\n    )\n    name: Mapped[str] = mapped_column(String(100))\n"
        },
        {
          "description": "Dual ID pattern (internal + public)",
          "code": "import uuid\nfrom sqlalchemy import Uuid, Integer\n\nclass User(Base):\n    __tablename__ = \"user\"\n    \n    # Internal ID for joins (faster)\n    id: Mapped[int] = mapped_column(Integer, primary_key=True)\n    \n    # Public ID for API (safe to expose)\n    public_id: Mapped[uuid.UUID] = mapped_column(\n        Uuid,\n        unique=True,\n        default=uuid.uuid4,\n        index=True\n    )\n    \n    name: Mapped[str] = mapped_column(String(100))\n"
        },
        {
          "description": "UUID7 for sortable UUIDs",
          "code": "import uuid\nfrom uuid6 import uuid7\nfrom sqlalchemy import Uuid\n\nclass Event(Base):\n    __tablename__ = \"event\"\n    \n    # UUID7 is time-sortable\n    id: Mapped[uuid.UUID] = mapped_column(\n        Uuid,\n        primary_key=True,\n        default=uuid7\n    )\n    event_type: Mapped[str] = mapped_column(String(50))\n"
        }
      ]
    },
    {
      "id": "db-base-model",
      "category": "database",
      "title": "Common Base Model Pattern",
      "tags": [
        "database",
        "sqlalchemy",
        "base-model",
        "mixin",
        "reusable"
      ],
      "summary": "Create a base model with common fields and methods.",
      "content": "Define a base model with common fields (id, timestamps) and methods (to_dict, repr). Use mixins for optional features like soft delete. This reduces boilerplate and ensures consistency.",
      "code_examples": [
        {
          "description": "Complete base model with mixins",
          "code": "from datetime import datetime\nfrom sqlalchemy import DateTime, Integer, func\nfrom sqlalchemy.orm import DeclarativeBase, Mapped, mapped_column\n\nclass Base(DeclarativeBase):\n    pass\n\nclass IDMixin:\n    id: Mapped[int] = mapped_column(Integer, primary_key=True)\n\nclass TimestampMixin:\n    created_at: Mapped[datetime] = mapped_column(\n        DateTime(timezone=True),\n        server_default=func.now()\n    )\n    updated_at: Mapped[datetime] = mapped_column(\n        DateTime(timezone=True),\n        server_default=func.now(),\n        onupdate=func.now()\n    )\n\nclass BaseModel(Base, IDMixin, TimestampMixin):\n    \"\"\"Abstract base model with ID and timestamps.\"\"\"\n    __abstract__ = True\n    \n    def to_dict(self) -> dict:\n        return {\n            c.name: getattr(self, c.name)\n            for c in self.__table__.columns\n        }\n    \n    def __repr__(self) -> str:\n        return f\"<{self.__class__.__name__}(id={self.id})>\"\n\nclass User(BaseModel):\n    __tablename__ = \"user\"\n    name: Mapped[str] = mapped_column(String(100))\n    email: Mapped[str] = mapped_column(String(255), unique=True)\n"
        }
      ]
    },
    {
      "id": "db-repository-pattern",
      "category": "database",
      "title": "Repository Pattern",
      "tags": [
        "database",
        "sqlalchemy",
        "repository",
        "architecture",
        "abstraction"
      ],
      "summary": "Abstract database operations behind a repository for cleaner code.",
      "content": "The repository pattern abstracts database operations, making code more testable and maintainable. Create base repository with common CRUD operations, extend for specific entities.",
      "code_examples": [
        {
          "description": "Generic async repository",
          "code": "from typing import Generic, TypeVar, Type\nfrom sqlalchemy import select\nfrom sqlalchemy.ext.asyncio import AsyncSession\n\nModelType = TypeVar(\"ModelType\", bound=Base)\n\nclass BaseRepository(Generic[ModelType]):\n    def __init__(self, session: AsyncSession, model: Type[ModelType]):\n        self.session = session\n        self.model = model\n    \n    async def get(self, id: int) -> ModelType | None:\n        return await self.session.get(self.model, id)\n    \n    async def get_all(self, skip: int = 0, limit: int = 100) -> list[ModelType]:\n        result = await self.session.scalars(\n            select(self.model).offset(skip).limit(limit)\n        )\n        return result.all()\n    \n    async def create(self, obj: ModelType) -> ModelType:\n        self.session.add(obj)\n        await self.session.commit()\n        await self.session.refresh(obj)\n        return obj\n    \n    async def update(self, obj: ModelType) -> ModelType:\n        await self.session.commit()\n        await self.session.refresh(obj)\n        return obj\n    \n    async def delete(self, obj: ModelType) -> None:\n        await self.session.delete(obj)\n        await self.session.commit()\n"
        },
        {
          "description": "Specific repository with custom methods",
          "code": "class UserRepository(BaseRepository[User]):\n    def __init__(self, session: AsyncSession):\n        super().__init__(session, User)\n    \n    async def get_by_email(self, email: str) -> User | None:\n        result = await self.session.scalar(\n            select(User).where(User.email == email)\n        )\n        return result\n    \n    async def get_active_users(self) -> list[User]:\n        result = await self.session.scalars(\n            select(User).where(User.is_active == True)\n        )\n        return result.all()\n\n# Usage in service\nclass UserService:\n    def __init__(self, repo: UserRepository):\n        self.repo = repo\n    \n    async def get_user_by_email(self, email: str) -> User:\n        user = await self.repo.get_by_email(email)\n        if not user:\n            raise UserNotFound()\n        return user\n"
        }
      ]
    },
    {
      "id": "db-testing-fixtures",
      "category": "database",
      "title": "Database Testing with Fixtures",
      "tags": [
        "database",
        "sqlalchemy",
        "testing",
        "pytest",
        "fixtures"
      ],
      "summary": "Set up proper database fixtures for testing with transaction rollback.",
      "content": "Use pytest fixtures with transaction rollback for fast, isolated tests. Create tables once, wrap each test in a transaction that rolls back. Use factories for test data.",
      "code_examples": [
        {
          "description": "Async database fixtures with rollback",
          "code": "import pytest\nfrom sqlalchemy.ext.asyncio import create_async_engine, AsyncSession, async_sessionmaker\n\n@pytest.fixture(scope=\"session\")\ndef event_loop():\n    import asyncio\n    loop = asyncio.get_event_loop_policy().new_event_loop()\n    yield loop\n    loop.close()\n\n@pytest.fixture(scope=\"session\")\nasync def engine():\n    engine = create_async_engine(\"postgresql+asyncpg://test:test@localhost/test\")\n    async with engine.begin() as conn:\n        await conn.run_sync(Base.metadata.create_all)\n    yield engine\n    async with engine.begin() as conn:\n        await conn.run_sync(Base.metadata.drop_all)\n    await engine.dispose()\n\n@pytest.fixture\nasync def session(engine):\n    async with engine.connect() as conn:\n        await conn.begin()  # Start transaction\n        \n        async_session = async_sessionmaker(\n            bind=conn, \n            class_=AsyncSession,\n            expire_on_commit=False\n        )\n        \n        async with async_session() as session:\n            yield session\n        \n        await conn.rollback()  # Rollback after each test\n"
        },
        {
          "description": "Factory for test data",
          "code": "from factory import Factory, Faker, LazyAttribute\nfrom factory.alchemy import SQLAlchemyModelFactory\n\nclass UserFactory(SQLAlchemyModelFactory):\n    class Meta:\n        model = User\n        sqlalchemy_session_persistence = \"flush\"\n    \n    name = Faker(\"name\")\n    email = LazyAttribute(lambda o: f\"{o.name.lower().replace(' ', '.')}@test.com\")\n    is_active = True\n\n# Usage in test\n@pytest.mark.asyncio\nasync def test_get_user(session):\n    UserFactory._meta.sqlalchemy_session = session\n    user = UserFactory()\n    await session.flush()\n    \n    result = await session.get(User, user.id)\n    assert result.name == user.name\n"
        }
      ]
    }
  ]
}
